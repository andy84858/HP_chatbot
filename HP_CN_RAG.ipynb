{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4965,
     "status": "ok",
     "timestamp": 1725344534383,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "rEzcs4nUT-1h",
    "outputId": "7e4a0573-eed2-4a7d-8f97-f515a7c23955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1725344534717,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "HS0zzn3-UNqZ",
    "outputId": "8e698342-8791-4e1c-a516-d50cf51045ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/Side Projects/HP_chat_bot\n",
      " bert_tune_for_context_cn.ipynb\n",
      " \u001b[0m\u001b[01;34mchinese_database\u001b[0m/\n",
      " data_preprocessing_for_bert.ipynb\n",
      " \u001b[01;34menglish_database\u001b[0m/\n",
      " find_context.ipynb\n",
      " \u001b[01;34mfine_tuned_ckiplab_bert_base_chinese\u001b[0m/\n",
      " harry_potter_chatbot.ipynb\n",
      " HP_1_CN.txt\n",
      " HP_1_EN.txt\n",
      " HP_2_CN.txt\n",
      " HP_2_EN.txt\n",
      " HP_3_CN.txt\n",
      " HP_3_EN.txt\n",
      " HP_4_CN.txt\n",
      " HP_4_EN.txt\n",
      " HP_5_CN.txt\n",
      " HP_5_EN.txt\n",
      " HP_6_CN.txt\n",
      " HP_6_EN.txt\n",
      " HP_7_CN.txt\n",
      " HP_7_EN.txt\n",
      " \u001b[01;34mhp_cn_database\u001b[0m/\n",
      " \u001b[01;34mhp_cn_database_4cc0126e18294303a4868ed7c6ba5ccf\u001b[0m/\n",
      " HP_CN_QA.csv\n",
      " HP_CN_RAG_1.ipynb\n",
      " HP_CN_RAG_2.ipynb\n",
      " HP_CN_RAG_3.ipynb\n",
      " HP_CN_RAG_4.ipynb\n",
      " HP_CN_RAG_5.ipynb\n",
      " HP_CN_RAG.ipynb\n",
      " \u001b[01;34mhp_en_database\u001b[0m/\n",
      " HP_EN_QA.csv\n",
      " HP_EN_RAG.ipynb\n",
      " \u001b[01;34mresults\u001b[0m/\n",
      " \u001b[01;34mtrained_ckiplab_bert_base_chinese\u001b[0m/\n",
      "'Transformers Training and Inference on Remote Hardware.ipynb'\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/Colab\\ Notebooks/Side Projects/HP_chat_bot\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24848,
     "status": "ok",
     "timestamp": 1725359063957,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "Y_n_EWRoUasm",
    "outputId": "21aab1c5-c5ce-4f67-f366-f59da84230d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.15 (from langchain-community)\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.37 (from langchain-community)\n",
      "  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.15->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.15->langchain-community) (2.8.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.37->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.37->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.37->langchain-community) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.37->langchain-community)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.15->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.15->langchain-community) (2.20.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
      "Downloading langchain_community-0.2.15-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.37-py3-none-any.whl (396 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.108-py3-none-any.whl (150 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.15 langchain-community-0.2.15 langchain-core-0.2.37 langchain-text-splitters-0.2.2 langsmith-0.1.108 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 tenacity-8.5.0 typing-inspect-0.9.0\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.37)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
      "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (0.1.108)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.8)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.35->langchain_openai) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
      "Downloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain_openai\n",
      "Successfully installed jiter-0.5.0 langchain_openai-0.1.23 openai-1.43.0 tiktoken-0.7.0\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community\n",
    "%pip install langchain_openai\n",
    "%pip install transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3292,
     "status": "ok",
     "timestamp": 1725359076219,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "rXh_z9SyUccM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1725346752735,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "LWXo6_tkdRIT",
    "outputId": "337cfb40-94fe-4986-a639-7bc724786e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access files\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "rOkVqsB2UeFm"
   },
   "outputs": [],
   "source": [
    "# Set Up OpenAI API Key\n",
    "API_KEY='YOUR_PERSONAL_OPENAI_API_KEY'\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "tBK_IZksa_hM"
   },
   "outputs": [],
   "source": [
    "# Define a custom embedding class using Microsoft's multilingual model\n",
    "class MicrosoftEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        # Initialize the tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "        self.model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-small\").to(device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Embed a list of texts using the model\n",
    "        all_embeddings = []\n",
    "        batch_size = 32\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding documents\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            all_embeddings.extend(embeddings.tolist())\n",
    "        return all_embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        # Embed a single query\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        return embeddings[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "MQlAjB5mp2Yh"
   },
   "outputs": [],
   "source": [
    "# Define a class for OpenAI embeddings with retry mechanism\n",
    "class OpenAIEmbeddingsWithRetry(OpenAIEmbeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        try:\n",
    "            return super().embed_documents(texts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in embedding: {e}\")\n",
    "            time.sleep(60)\n",
    "            return super().embed_documents(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        try:\n",
    "            return super().embed_query(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in embedding query: {e}\")\n",
    "            time.sleep(60)\n",
    "            return super().embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "AquIFKZxVRBe"
   },
   "outputs": [],
   "source": [
    "# Function to load documents\n",
    "def load_documents():\n",
    "    cn_files = [f'HP_{i}_CN.txt' for i in range(1, 8)]\n",
    "    # Initialize an empty list to store all documents\n",
    "    documents = []\n",
    "    for file_path in cn_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                doc = Document(page_content=content, metadata={\"source\": file_path})\n",
    "                documents.append(doc)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "sFKFoe0OVWHs"
   },
   "outputs": [],
   "source": [
    "# Function to process and store documents in Chroma database\n",
    "def process_and_store_documents(embedding_function, force_recreate=False):\n",
    "    # Create database path\n",
    "    CHROMA_PATH = 'hp_cn_database'\n",
    "\n",
    "    # Check if database is already existed\n",
    "    if os.path.exists(CHROMA_PATH) and not force_recreate:\n",
    "        print(\"Loading existing database...\")\n",
    "        return Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function), CHROMA_PATH\n",
    "\n",
    "    print(\"Creating new database...\")\n",
    "    # Delete old database if need to rebuild database\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new Chroma database\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,                                             # Increase chunk_size to include more chinese sentence\n",
    "        chunk_overlap=100,                                          # Increase overlap to keep consistency of context\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\", \" \", \"\"],  # Add common chinese punctuation as separators\n",
    "        keep_separator=True,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents()\n",
    "    # Split Data Into Chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No documents to process. Check your document loading and splitting.\")\n",
    "\n",
    "    print(f\"Total chunks to process: {len(chunks)}\")\n",
    "\n",
    "\n",
    "    # Initialize db\n",
    "    db = None\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing document chunks\"):\n",
    "        batch_chunks = chunks[i:i+batch_size]\n",
    "        try:\n",
    "            if db is None:\n",
    "                db = Chroma.from_documents(batch_chunks, embedding_function, persist_directory=CHROMA_PATH)\n",
    "            else:\n",
    "                db.add_documents(batch_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}-{i+batch_size}: {e}\")\n",
    "            time.sleep(60)\n",
    "        time.sleep(1)\n",
    "\n",
    "    if db is None:\n",
    "        raise ValueError(\"Failed to create database. Check your embedding function and document processing.\")\n",
    "\n",
    "    db.persist()\n",
    "    return db, CHROMA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1725346752736,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "Ggq-zXjeVYgS"
   },
   "outputs": [],
   "source": [
    "# Function to search and generate responses\n",
    "def search_and_generate(query, db):\n",
    "    results = db.similarity_search_with_relevance_scores(query, k=5)\n",
    "    context = \"\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"根據以下上下文以繁體中文回答問題：\\n\\n{context}\\n\\n問題：{query}\\n\\n請確保回答與哈利波特的故事情節一致，並且只基於给定的上下文。\"\n",
    "    )\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    response = chat_model.invoke(prompt)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303292,
     "status": "ok",
     "timestamp": 1725347291867,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "FTxTzB5dVcEM",
    "outputId": "4b86513b-1a04-44fa-9190-aa92c5cc9d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Do you want to recreate the database? (y/n): n\n",
      "Choose embedding model (openai/microsoft): openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-c0e14a080598>:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  return Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function), CHROMA_PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing database...\n",
      "Database ready. Starting conversation...\n",
      "请输入您的问题（输入 'quit' 退出）: 哈利在三巫鬥法大賽中，吃了什麼讓他長出了腮，因而可以在水中游泳？\n",
      "回答: 哈利在三巫鬥法大賽中吃了「海洋草」，這使得他長出了腮，能夠在水中游泳。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 傳說中的混血王子是誰？\n",
      "回答: 傳說中的混血王子是塞弗勒斯·斯內普（Severus Snape）。他在《哈利·波特》系列中被稱為混血王子，因為他的父親是麻瓜，而母親則是巫師。這個稱號源於他在學校時期的身份以及他的血統背景。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 誰教會了哈利·波特護法咒？\n",
      "回答: 哈利·波特是由小天狼星·布萊克教會護法咒的。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 霍格華茲各學院的學院杯分數是用什麼記錄的？\n",
      "回答: 霍格華茲各學院的學院杯分數是用學院的獎勵和懲罰系統來記錄的。學生可以通過獲得優秀成績、參加競賽或表現良好來獲得分數，而違規或不當行為則會導致扣分。這些分數會隨著學年的進行而累積，最終決定哪一個學院獲得學院杯。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 小天狼星·布萊克的外號有人知道嗎？\n",
      "回答: 小天狼星·布萊克的外號是「小天狼星」。在《哈利·波特》系列中，他是一位著名的巫師，曾經是個追隨者的「死神」之一，後來成為哈利·波特的教父。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 誰終結了娜吉妮？\n",
      "回答: 在哈利波特的故事中，娜吉妮是伏地魔的仆從和一條蛇。終結娜吉妮的是哈利·波特，他在最後的戰鬥中使用了榮恩·衛斯理的劍，成功地殺死了她。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 霍格華茲女生盥洗室裡游蕩著哪位幽靈？\n",
      "回答: 霍格華茲女生盥洗室裡游蕩著的是「悲傷的女鬼」（Moaning Myrtle）。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 哈利的第一根魔杖杖芯是什麼？\n",
      "回答: 哈利的第一根魔杖杖芯是鳳凰羽毛。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 孚立維教授用什麼方法保護魔法石？\n",
      "回答: 孚立維教授用了多重魔法來保護魔法石，包括設置一些具有挑戰性的魔法障礙和守護生物。他還使用了他的專業知識設計出了一系列的考驗，只有具備足夠智慧和勇氣的人才能通過，這樣才能確保魔法石不會落入壞人之手。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 被食死人抓住後，是誰給哈利波特施了蜇人咒？\n",
      "回答: 在《哈利·波特與密室》中，被食死人抓住後，是小天狼星·布萊克施了蜇人咒。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 波巴洞的校長馬克沁夫人（台譯：歐琳·美心夫人）說她的馬只喝什麼？\n",
      "回答: 波巴洞的校長馬克沁夫人說她的馬只喝「阿根廷的馬爾貝克紅酒」。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 波巴洞的校長歐琳·美心夫人說她的馬只喝什麼？\n",
      "回答: 根據上下文，波巴洞的校長歐琳·美心夫人說她的馬只喝「純淨的水」。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 與哈利波特交好的衛斯理一家曾居住的房子叫什麼？\n",
      "回答: 衛斯理一家曾居住的房子叫做「福克斯之家」。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 如果妙麗參加了三巫鬥法大賽，哈利會一起參加嗎？\n",
      "回答: 根據《哈利·波特與火焰杯》的情節，如果妙麗參加了三巫鬥法大賽，哈利是有可能會一起參加的。事實上，在故事中，哈利和妙麗都是霍格華茲的學生，並且哈利最終也參加了這個比賽。因此可以推斷，若妙麗參加，哈利也會有可能隨之參加。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: 假瘋眼穆迪在懲罰馬份時，把他變成了？\n",
      "回答: 假瘋眼穆迪在懲罰馬份時，把他變成了一隻豬。\n",
      "\n",
      "请输入您的问题（输入 'quit' 退出）: quit\n",
      "Do you want to keep the database for future use? (y/n): y\n",
      "Database retained for future use.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Initializing...\")\n",
    "\n",
    "    # Check if need to rebuild Chroma database\n",
    "    force_recreate = input(\"Do you want to recreate the database? (y/n): \").lower().strip() == 'y'\n",
    "\n",
    "    # Choose embedding model\n",
    "    embedding_choice = input(\"Choose embedding model (openai/microsoft): \").lower().strip()\n",
    "    if embedding_choice == \"openai\":\n",
    "        embedding_function = OpenAIEmbeddingsWithRetry(model=\"text-embedding-3-small\")\n",
    "    elif embedding_choice == \"microsoft\":\n",
    "        embedding_function = MicrosoftEmbeddings()\n",
    "    else:\n",
    "        print(\"Invalid choice. Using Microsoft embeddings by default.\")\n",
    "        embedding_function = MicrosoftEmbeddings()\n",
    "\n",
    "    db, db_path = process_and_store_documents(embedding_function, force_recreate)\n",
    "    print(\"Database ready. Starting conversation...\")\n",
    "\n",
    "    # Keep asking question until input 'quit'\n",
    "    while True:\n",
    "        query = input(\"請輸入您的問題（輸入 'quit' 退出）: \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        answer = search_and_generate(query, db)\n",
    "        print(f\"回答: {answer}\\n\")\n",
    "\n",
    "    # Keep Chroma database if needed\n",
    "    keep_db = input(\"Do you want to keep the database for future use? (y/n): \").lower().strip() == 'y'\n",
    "    if not keep_db:\n",
    "        db.delete_collection()\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "        print(\"Database cleaned up.\")\n",
    "    else:\n",
    "        print(\"Database retained for future use.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1725346952707,
     "user": {
      "displayName": "林亮穎",
      "userId": "13258537141148788516"
     },
     "user_tz": -480
    },
    "id": "YmAuLye0Y-lf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZ6qDxamqLMwfqdNvnsYtY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
